{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9774ad5-d762-467a-a054-2796e6123cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 15:14:35,978 - INFO - CUDA Device: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "2024-11-27 15:14:35,978 - INFO - Available VRAM: 8.00GB\n",
      "2024-11-27 15:14:37,843 - INFO - Loading model with optimized settings for RTX 4070...\n",
      "2024-11-27 15:14:45,634 - INFO - Model loaded successfully with optimal settings\n",
      "2024-11-27 15:14:45,642 - INFO - Initialized TranslationApp using device: cuda\n",
      "2024-11-27 15:14:45,643 - INFO - Translation app started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Japanese-English Translation App (NLLB 1.3B) ===\n",
      "1. Translate text\n",
      "2. Fine-tune model\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Select an option (1-3):  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thank you for using the translation app!\n"
     ]
    }
   ],
   "source": [
    "from transformers import NllbTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "from dataclasses import dataclass\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('translation_app.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class TranslationExample:\n",
    "    \"\"\"Data class for storing translation pairs\"\"\"\n",
    "    japanese: str\n",
    "    english: str\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"Dataset class for handling translation data\"\"\"\n",
    "    def __init__(self, japanese_texts: List[str], english_texts: List[str], tokenizer: NllbTokenizer):\n",
    "        self.japanese_texts = japanese_texts\n",
    "        self.english_texts = english_texts\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.japanese_texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        jp_text = self.japanese_texts[idx]\n",
    "        en_text = self.english_texts[idx]\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            f\"jpn_Jpan {jp_text}\", \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                f\"eng_Latn {en_text}\", \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "class TranslationApp:\n",
    "    \"\"\"Translation application optimized for RTX 4070 Laptop GPU\"\"\"\n",
    "    def __init__(self, feedback_file: str = \"translation_feedback.json\"):\n",
    "        self.model_name = \"facebook/nllb-200-1.3B\"\n",
    "        self.feedback_file = Path(feedback_file)\n",
    "        \n",
    "        # Initialize device and optimize GPU settings\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if self.device.type == \"cuda\":\n",
    "            # Optimize for 8GB VRAM\n",
    "            torch.cuda.set_per_process_memory_fraction(0.85)\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            logging.info(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "            logging.info(f\"Available VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB\")\n",
    "        \n",
    "        self.initialize_model()\n",
    "        self.feedback_data = self.load_feedback()\n",
    "        logging.info(f\"Initialized TranslationApp using device: {self.device}\")\n",
    "\n",
    "    def initialize_model(self) -> None:\n",
    "        \"\"\"Initialize model with RTX 4070 optimizations\"\"\"\n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.tokenizer = NllbTokenizer.from_pretrained(self.model_name)\n",
    "            \n",
    "            # Optimized loading for 8GB VRAM\n",
    "            logging.info(\"Loading model with optimized settings for RTX 4070...\")\n",
    "            \n",
    "            # Create offload folder\n",
    "            os.makedirs(\"offload_folder\", exist_ok=True)\n",
    "            \n",
    "            model_kwargs = {\n",
    "                \"device_map\": \"auto\",\n",
    "                \"torch_dtype\": torch.float16,\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "                \"max_memory\": {0: \"7GiB\", \"cpu\": \"12GiB\"},\n",
    "                \"offload_folder\": \"offload_folder\"\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    **model_kwargs\n",
    "                )\n",
    "                logging.info(\"Model loaded successfully with optimal settings\")\n",
    "                \n",
    "            except ImportError:\n",
    "                logging.warning(\"Accelerate library not found. Installing...\")\n",
    "                os.system(\"pip install 'accelerate>=0.26.0'\")\n",
    "                self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    **model_kwargs\n",
    "                )\n",
    "            \n",
    "            # Set language codes\n",
    "            self.src_lang = \"jpn_Jpan\"\n",
    "            self.tgt_lang = \"eng_Latn\"\n",
    "            self.tgt_lang_id = self.tokenizer.convert_tokens_to_ids(self.tgt_lang)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Optimal loading failed: {e}\")\n",
    "            self._handle_model_loading_fallback(e)\n",
    "\n",
    "    def _handle_model_loading_fallback(self, error: Exception) -> None:\n",
    "        \"\"\"Fallback loading with memory optimizations\"\"\"\n",
    "        try:\n",
    "            logging.info(\"Attempting to load with basic float16...\")\n",
    "            self.model = (\n",
    "                AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                .to(self.device)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Float16 loading failed: {e}\")\n",
    "            logging.info(\"Falling back to basic model loading...\")\n",
    "            self.model = (\n",
    "                AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                .to(self.device)\n",
    "            )\n",
    "\n",
    "    def _monitor_memory(self) -> None:\n",
    "        \"\"\"Monitor memory usage\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            vram_used = torch.cuda.memory_allocated() / 1024**3\n",
    "            vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            logging.info(f\"VRAM Usage: {vram_used:.2f}GB / {vram_total:.2f}GB\")\n",
    "        \n",
    "        ram_used = psutil.Process(os.getpid()).memory_info().rss / 1024**3\n",
    "        logging.info(f\"RAM Usage: {ram_used:.2f}GB\")\n",
    "\n",
    "    def load_feedback(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Load feedback data\"\"\"\n",
    "        if self.feedback_file.exists():\n",
    "            try:\n",
    "                with open(self.feedback_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                logging.error(f\"Error reading feedback file: {self.feedback_file}\")\n",
    "        return {\"japanese\": [], \"english\": []}\n",
    "\n",
    "    def save_feedback(self) -> None:\n",
    "        \"\"\"Save feedback data\"\"\"\n",
    "        try:\n",
    "            with open(self.feedback_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.feedback_data, f, ensure_ascii=False, indent=2)\n",
    "            logging.info(\"Feedback saved successfully\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving feedback: {e}\")\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def translate(self, japanese_text: str, beam_size: int = 5) -> str:\n",
    "        \"\"\"Optimized translation for RTX 4070\"\"\"\n",
    "        try:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            self._monitor_memory()\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                f\"{self.src_lang} {japanese_text}\",\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "            ).to(self.device)\n",
    "            \n",
    "            generated_tokens = self.model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=self.tgt_lang_id,\n",
    "                max_length=128,\n",
    "                num_beams=beam_size,\n",
    "                length_penalty=0.8,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,\n",
    "                num_beam_groups=beam_size,\n",
    "                diversity_penalty=0.1,\n",
    "                do_sample=False\n",
    "            )\n",
    "            \n",
    "            translation = self.tokenizer.batch_decode(\n",
    "                generated_tokens,\n",
    "                skip_special_tokens=True\n",
    "            )[0].replace(f\"{self.tgt_lang} \", \"\")\n",
    "            \n",
    "            return translation\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Translation error: {e}\")\n",
    "            return f\"Error during translation: {str(e)}\"\n",
    "        finally:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    def add_feedback(self, japanese_text: str, correct_english: str) -> None:\n",
    "        \"\"\"Add translation feedback\"\"\"\n",
    "        if not japanese_text.strip() or not correct_english.strip():\n",
    "            logging.warning(\"Empty translation pair received\")\n",
    "            return\n",
    "        \n",
    "        self.feedback_data[\"japanese\"].append(japanese_text)\n",
    "        self.feedback_data[\"english\"].append(correct_english)\n",
    "        self.save_feedback()\n",
    "\n",
    "    def fine_tune(self, epochs: int = 2, batch_size: int = 1, \n",
    "                 learning_rate: float = 2e-6) -> None:\n",
    "        \"\"\"Fine-tune with memory optimizations for RTX 4070\"\"\"\n",
    "        if len(self.feedback_data[\"japanese\"]) == 0:\n",
    "            logging.warning(\"No feedback data available\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            self._monitor_memory()\n",
    "            \n",
    "            dataset = TranslationDataset(\n",
    "                self.feedback_data[\"japanese\"],\n",
    "                self.feedback_data[\"english\"],\n",
    "                self.tokenizer\n",
    "            )\n",
    "            dataloader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            # Gradient accumulation steps\n",
    "            gradient_accumulation_steps = max(1, 4 // batch_size)\n",
    "            \n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.model.parameters(),\n",
    "                lr=learning_rate,\n",
    "                weight_decay=0.01\n",
    "            )\n",
    "            \n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.5, patience=1, verbose=True\n",
    "            )\n",
    "            \n",
    "            self.model.train()\n",
    "            best_loss = float('inf')\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                total_loss = 0\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                for batch_idx, batch in enumerate(dataloader):\n",
    "                    try:\n",
    "                        batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                        \n",
    "                        outputs = self.model(**batch)\n",
    "                        loss = outputs.loss / gradient_accumulation_steps\n",
    "                        loss.backward()\n",
    "                        \n",
    "                        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "                        \n",
    "                        total_loss += loss.item() * gradient_accumulation_steps\n",
    "                        \n",
    "                        if batch_idx % 2 == 0:\n",
    "                            self._monitor_memory()\n",
    "                            logging.info(\n",
    "                                f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}, \"\n",
    "                                f\"Loss: {loss.item() * gradient_accumulation_steps:.4f}\"\n",
    "                            )\n",
    "                            \n",
    "                    except RuntimeError as e:\n",
    "                        if \"out of memory\" in str(e):\n",
    "                            if torch.cuda.is_available():\n",
    "                                torch.cuda.empty_cache()\n",
    "                            logging.error(f\"OOM error in batch {batch_idx}. Skipping...\")\n",
    "                            optimizer.zero_grad()\n",
    "                            continue\n",
    "                        raise e\n",
    "                \n",
    "                avg_loss = total_loss / len(dataloader)\n",
    "                scheduler.step(avg_loss)\n",
    "                \n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    logging.info(f\"New best loss: {best_loss:.4f}\")\n",
    "                \n",
    "                logging.info(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            self.model.eval()\n",
    "            logging.info(\"Fine-tuning complete!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Fine-tuning error: {e}\")\n",
    "        finally:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with improved error handling\"\"\"\n",
    "    try:\n",
    "        app = TranslationApp()\n",
    "        logging.info(\"Translation app started\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                print(\"\\n=== Japanese-English Translation App (NLLB 1.3B) ===\")\n",
    "                print(\"1. Translate text\")\n",
    "                print(\"2. Fine-tune model\")\n",
    "                print(\"3. Exit\")\n",
    "                \n",
    "                choice = input(\"\\nSelect an option (1-3): \").strip()\n",
    "                \n",
    "                if choice == '1':\n",
    "                    japanese_text = input(\"\\nEnter Japanese text: \").strip()\n",
    "                    if not japanese_text:\n",
    "                        print(\"Please enter some text to translate.\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(\"\\nTranslating...\")\n",
    "                    translation = app.translate(japanese_text)\n",
    "                    print(f\"\\nTranslation: {translation}\")\n",
    "                    \n",
    "                    feedback = input(\"\\nIs this translation correct? (y/n): \").strip().lower()\n",
    "                    if feedback == 'n':\n",
    "                        correct_translation = input(\"Please provide the correct translation: \").strip()\n",
    "                        if correct_translation:\n",
    "                            app.add_feedback(japanese_text, correct_translation)\n",
    "                    \n",
    "                elif choice == '2':\n",
    "                    if len(app.feedback_data[\"japanese\"]) == 0:\n",
    "                        print(\"\\nNo feedback data available for fine-tuning.\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"\\nAvailable feedback pairs: {len(app.feedback_data['japanese'])}\")\n",
    "                    confirm = input(\"Start fine-tuning? (y/n): \").strip().lower()\n",
    "                    if confirm == 'y':\n",
    "                        print(\"\\nStarting fine-tuning process...\")\n",
    "                        app.fine_tune()\n",
    "                    \n",
    "                elif choice == '3':\n",
    "                    print(\"\\nThank you for using the translation app!\")\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    print(\"\\nInvalid option. Please try again.\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nOperation cancelled by user.\")\n",
    "                continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in main loop: {e}\")\n",
    "                print(\"An error occurred. Please try again.\")\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting...\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Critical error: {e}\")\n",
    "        print(\"A critical error occurred. Please check the logs.\")\n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
